{"version":3,"sources":["@stdlib/ml/online-sgd-regression/lib/dot.js","@stdlib/ml/online-sgd-regression/lib/weight_vector.js","@stdlib/ml/online-sgd-regression/lib/regularize.js","@stdlib/ml/online-sgd-regression/lib/loss/epsilon_insensitive.js","@stdlib/ml/online-sgd-regression/lib/loss/squared_error.js","@stdlib/ml/online-sgd-regression/lib/loss/huber.js","@stdlib/ml/online-sgd-regression/lib/eta_factory.js","@stdlib/ml/online-sgd-regression/lib/validate.js","@stdlib/ml/online-sgd-regression/lib/learner.js","@stdlib/ml/online-sgd-regression/lib/index.js"],"names":["dot","x","y","i","len","length","ret","isPositiveInteger","__var_0__","isBoolean","__var_1__","setReadOnly","__var_2__","pow","__var_3__","__var_4__","MIN_SCALE","scaleTo","factor","this","scale","nWeights","_data","norm","RangeError","add","xScale","xscaled","inner","intercept","innerProduct","WeightVector","dim","TypeError","Array","prototype","max","MIN_SCALING_FACTOR","regularize","weights","lambda","eta","scalingFactor","epsilonInsensitiveLoss","epsilon","p","squaredErrorLoss","loss","huberLoss","closure","type","eta0","iter","Error","isNonNegativeNumber","isPrimitive","isPositiveNumber","isObject","isString","hasOwnProp","__var_5__","validate","opts","options","learningRate","isArray","copy","defineProperty","__var_6__","getEta","__var_7__","DEFAULTS","__var_8__","__var_9__","onlineSGDRegression","_nFeatures","_lossfun","_weights","_getEta","model","err","arguments","get","configurable","enumerable","update","nFeatures","predict"],"mappings":"0lBAoCA,SAASA,EAAKC,EAAGC,GAChB,IAEIC,EAFAC,EAAMH,EAAEI,OACRC,EAAM,EAGV,IAAMH,EAAI,EAAGA,EAAIC,EAAKD,IACrBG,GAAOL,EAAGE,GAAMD,EAAGC,GAEpB,OAAOG,ECdR,IAAIC,EAAoBC,EACpBC,EAAYC,EACZC,EAAcC,EACdC,EAAMC,EACNd,EAAMe,EAKNC,EAAY,MAYhB,SAASC,EAASC,GAEjB,IAAIf,EACJ,GAAKgB,KAAKC,MAAQJ,EAAY,CAE7B,IAAMb,EAAI,EAAGA,EAAIgB,KAAKE,SAAUlB,IAC/BgB,KAAKG,MAAOnB,IAAOgB,KAAKC,MAEzBD,KAAKC,MAAQ,EAKd,GAFAD,KAAKI,MAAQV,EAAKK,EAAQ,KAErBA,EAAS,GAGb,MAAM,IAAIM,WAAY,sGAAwGN,EAAS,KAFvIC,KAAKC,OAASF,EAahB,SAASO,EAAKxB,EAAGyB,GAEhB,IAAIC,EACAC,EACAzB,EAMJ,IAJAyB,EAAQ,OACQ,IAAXF,IACJA,EAAS,GAEJvB,EAAI,EAAGA,EAAIF,EAAEI,OAAQF,IAC1BwB,EAAU1B,EAAGE,GAAMuB,EACnBE,GAAST,KAAKG,MAAMnB,GAAKwB,EACzBR,KAAKG,MAAOnB,GAAMgB,KAAKG,MAAOnB,GAAQwB,EAAUR,KAAKC,MAGjDD,KAAKU,YACTF,EAAU,EAAMD,EAChBE,GAAST,KAAKG,MAAOnB,GAAMwB,EAC3BR,KAAKG,MAAOnB,GAAMgB,KAAKG,MAAOnB,GAAQwB,EAAUR,KAAKC,OAEtDD,KAAKI,OAAYvB,EAAKC,EAAGA,IAAUkB,KAAc,UAAK,EAAM,IAC3DN,EAAKa,EAAQ,GACX,EAAMP,KAAKC,MAAQQ,EAUvB,SAASE,EAAc7B,GAEtB,IACIE,EADAG,EAAM,EAEV,IAAMH,EAAI,EAAGA,EAAIF,EAAEI,OAAQF,IAC1BG,GAAOa,KAAKG,MAAOnB,GAAMF,EAAGE,GAI7B,OAFAG,GAASa,KAAc,UAAKA,KAAKG,MAAOnB,GAAM,EAC9CG,GAAOa,KAAKC,MAgBb,SAASW,EAAcC,EAAKH,GAC3B,IAAI1B,EACJ,KAAOgB,gBAAgBY,GACtB,OAAO,IAAIA,EAAcC,EAAKH,GAE/B,IAAMtB,EAAmByB,GACxB,MAAM,IAAIC,UAAW,8EAAgFD,EAAM,MAE5G,IAAMvB,EAAWoB,GAChB,MAAM,IAAII,UAAW,sFAAwFJ,EAAY,MAW1H,IARAV,KAAKC,MAAQ,EACbD,KAAKI,KAAO,EACZJ,KAAKU,UAAYA,EACjBV,KAAKE,SAAWW,GAAUb,KAAc,UAAK,EAAI,GAEjDA,KAAKG,MAAQ,IAAIY,MAAOf,KAAKE,UAGvBlB,EAAI,EAAGA,EAAIgB,KAAKE,SAAUlB,IAC/BgB,KAAKG,MAAOnB,GAAM,EAYpBQ,EAAaoB,EAAaI,UAAW,UAAWlB,GAUhDN,EAAaoB,EAAaI,UAAW,MAAOV,GAU5Cd,EAAaoB,EAAaI,UAAW,eAAgBL,GCtKrD,IAAIM,EAAM5B,EAKN6B,EAAqB,KAazB,SAASC,EAAYC,EAASC,EAAQC,GACrC,IAAIC,EACCF,EAAS,IACbE,EAAgB,EAAQD,EAAMD,EAC9BD,EAAQtB,QAASmB,EAAKM,EAAeL,KCtBvC,IAAIC,EAAa9B,EAoBjB,SAASmC,EAAwBJ,EAAStC,EAAGC,EAAGuC,EAAKD,EAAQI,GAC5D,IAAIC,EAAIN,EAAQT,aAAc7B,GAAMC,EAGpCoC,EAAYC,EAASC,EAAQC,GAExBI,EAAID,EACRL,EAAQd,IAAKxB,GAAIwC,GACNI,GAAKD,GAChBL,EAAQd,IAAKxB,GAAIwC,GC7BnB,IAAIH,EAAa9B,EAmBjB,SAASsC,EAAkBP,EAAStC,EAAGC,EAAGuC,EAAKD,GAC9C,IAAIO,EAAO7C,EAAIqC,EAAQT,aAAc7B,GAGrCqC,EAAYC,EAASC,EAAQC,GAE7BF,EAAQd,IAAKxB,EAAKwC,EAAMM,GCzBzB,IAAIT,EAAa9B,EAsBjB,SAASwC,EAAWT,EAAStC,EAAGC,EAAGuC,EAAKD,EAAQI,GAC/C,IAAIC,EAAIN,EAAQT,aAAc7B,GAAMC,EAGpCoC,EAAYC,EAASC,EAAQC,GAExBI,EAAID,EACRL,EAAQd,IAAKxB,GAAIwC,GACNI,GAAKD,EAChBL,EAAQd,IAAKxB,GAAIwC,GAEjBF,EAAQd,IAAKxB,GAAIwC,EAAMI,GCxBzB,SAASI,EAASC,EAAMC,EAAMX,GAC7B,IAAIY,EACA9C,EAIJ,OAFA8C,EAAO,EAEEF,GACT,IAAK,QAEJ5C,EAmBD,WACC,IAAImC,EAAM,KAAWW,EAAO,KAE5B,OADAA,GAAQ,EACDX,GArBP,MACD,IAAK,WACJnC,EA4BD,WAEC,OADA8C,GAAQ,EACDD,GA7BP,MACD,IAAK,UACJ7C,EAoCD,WACC,IAAImC,EAAM,GAAQD,EAASY,GAE3B,OADAA,GAAQ,EACDX,GAtCP,MACD,QACC,MAAM,IAAIY,MAAO,gGAAkGH,EAAO,KAE3H,OAAO5C,wHCxBJgD,EAAsB9C,EAAU+C,YAChCC,EAAmB9C,EAAU6C,YAC7B9C,EAAYG,EAAU2C,YACtBE,EAAW3C,EACX4C,EAAW3C,EAAUwC,YACrBI,EAAaC,EA2BjB,SAASC,EAAUC,EAAMC,GACxB,OAAMN,EAAUM,GAGXJ,EAAYI,EAAS,aACzBD,EAAKlB,QAAUmB,EAAQnB,SACjBY,EAAkBM,EAAKlB,UACrB,IAAIX,UAAW,wEAA0E6B,EAAKlB,QAAU,MAG5Ge,EAAYI,EAAS,UACzBD,EAAKX,KAAOY,EAAQZ,MACdK,EAAkBM,EAAKX,OACrB,IAAIlB,UAAW,qEAAuE6B,EAAKX,KAAO,MAGtGQ,EAAYI,EAAS,YACzBD,EAAKtB,OAASuB,EAAQvB,QAChBc,EAAqBQ,EAAKtB,SACxB,IAAIP,UAAW,0EAA4E6B,EAAKtB,OAAS,MAG7GmB,EAAYI,EAAS,kBACzBD,EAAKE,aAAeD,EAAQC,cACtBN,EAAUI,EAAKE,eACb,IAAI/B,UAAW,8EAAgF6B,EAAKE,aAAe,MAGvHL,EAAYI,EAAS,UACzBD,EAAKf,KAAOgB,EAAQhB,MACdW,EAAUI,EAAKf,OACb,IAAId,UAAW,sEAAwE6B,EAAKf,KAAO,MAGvGY,EAAYI,EAAS,eACzBD,EAAKjC,UAAYkC,EAAQlC,WACnBpB,EAAWqD,EAAKjC,YACd,IAAII,UAAW,4EAA8E6B,EAAKjC,UAAY,MAGhH,KAtCC,IAAII,UAAW,wDAA0D8B,EAAU,MC7B5F,IAAIE,EAAUzD,EACV0D,EAAOxD,EACPyD,EAAiBvD,EACjBmB,EAAejB,EACf6B,EAAyB5B,EACzB+B,EAAmBc,EACnBZ,GAAYoB,EACZC,GAASC,EACTC,GAAWC,EACXX,GAAWY,EA8Cf,SAASC,GAAqBX,GAC7B,IAAIY,EACAC,EACAC,EACAC,EACAC,EACAjB,EACAkB,EAGJ,GADAlB,EAAOI,EAAMK,IACRU,UAAU5E,OAAS,IACvB2E,EAAMnB,GAAUC,EAAMC,IAErB,MAAMiB,EAMR,OAHAH,EAAW,KAGFf,EAAKf,MACd,IAAK,qBACJ6B,EAAWjC,EACZ,MACA,IAAK,QACJiC,EAAW5B,GACZ,MACA,IAAK,eACJ4B,EAAW9B,EACZ,MACA,QACC,MAAMO,MAAO,8GAAgHS,EAAKf,KAAO,KAoG1I,OAhGA+B,EAAUT,GAAQP,EAAKE,aAAcF,EAAKX,KAAMW,EAAKtB,QAsBrD2B,EAfAY,EAAQ,GAee,QAAS,CAC/BG,IAAO,WACN,IAAI5E,EACAH,EAGJ,IADAG,EAAM,IAAI4B,MAAO2C,EAASxD,UACpBlB,EAAI,EAAGA,EAAIG,EAAID,OAAQF,IAC5BG,EAAKH,GAAM0E,EAASvD,MAAOnB,GAAM0E,EAASzD,MAE3C,OAAOd,GAER6E,cAAgB,EAChBC,YAAc,IAoBfL,EAAMM,OAAS,SAAiBpF,EAAGC,GAClC,IAAIuC,EAOJ,GALMoC,IACLA,EAAW,IAAI9C,EAAc9B,EAAEI,OAAQyD,EAAKjC,WAC5C8C,EAAab,EAAKjC,UAAYgD,EAASxD,SAAW,EAAIwD,EAASxD,WAG1D4C,EAAShE,IAAOA,EAAEI,SAAWsE,EAClC,MAAM,IAAI1C,UAAW,sEAAwEd,KAAKmE,UAAY,aAAerF,EAAI,KAIlIwC,EAAMqC,IAGNF,EAAUC,EAAU5E,EAAGC,EAAGuC,EAAKqB,EAAKtB,OAAQsB,EAAKlB,UAmBlDmC,EAAMQ,QAAU,SAAkBtF,GACjC,IAAMgE,EAAShE,IAAOA,EAAEI,SAAWsE,EAClC,MAAM,IAAI1C,UAAW,sEAAwEd,KAAKmE,UAAY,aAAerF,EAAI,KAElI,OAAO4E,EAAS/C,aAAc7B,IAGxB8E,ECjLL,IAACL,GAAsBlE","sourcesContent":["\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Calculates the dot product of two vectors.\n*\n* @private\n* @param {NumericArray} x - first vector\n* @param {NumericArray} y - second vector\n* @returns {number} dot product\n*\n* @example\n* var x = [ 1.0, 2.0, 3.0 ];\n* var y = [ 1.0, 2.0, 2.0 ];\n*\n* var ret = dot( x, y );\n* // returns 11.0\n*/\nfunction dot( x, y ) {\n\tvar len = x.length;\n\tvar ret = 0;\n\tvar i;\n\n\tfor ( i = 0; i < len; i++ ) {\n\t\tret += x[ i ] * y[ i ];\n\t}\n\treturn ret;\n}\n\n\n// EXPORTS //\nexport default dot;\n","import __var_0__ from '@stdlib/assert/is-positive-integer';\nimport __var_1__ from '@stdlib/assert/is-boolean';\nimport __var_2__ from '@stdlib/utils/define-nonenumerable-read-only-property';\nimport __var_3__ from '@stdlib/math/base/special/pow';\nimport __var_4__ from './dot.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/*\n* The weight vector implementation was inspired by the [sofia-ml]{@link https://code.google.com/archive/p/sofia-ml/} library.\n*/\n\n// MODULES //\nvar isPositiveInteger = __var_0__;\nvar isBoolean = __var_1__;\nvar setReadOnly = __var_2__;\nvar pow = __var_3__;\nvar dot = __var_4__;\n\n\n// VARIABLES //\n\nvar MIN_SCALE = 1.0e-11;\n\n\n// FUNCTIONS //\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @private\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nfunction scaleTo( factor ) {\n\t/* eslint-disable no-invalid-this */\n\tvar i;\n\tif ( this.scale < MIN_SCALE ) {\n\t\t// Scale vector to one:\n\t\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\t\tthis._data[ i ] *= this.scale;\n\t\t}\n\t\tthis.scale = 1.0;\n\t}\n\n\tthis.norm *= pow( factor, 2 );\n\n\tif ( factor > 0.0 ) {\n\t\tthis.scale *= factor;\n\t} else {\n\t\tthrow new RangeError( 'Scaling weight vector by nonpositive value, likely due to too large value of eta * lambda. Value: `' + factor + '`' );\n\t}\n}\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @private\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nfunction add( x, xScale ) {\n\t/* eslint-disable no-invalid-this */\n\tvar xscaled;\n\tvar inner;\n\tvar i;\n\n\tinner = 0.0;\n\tif ( xScale === void 0 ) {\n\t\txScale = 1.0;\n\t}\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\txscaled = x[ i ] * xScale;\n\t\tinner += this._data[i] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this.intercept ) {\n\t\txscaled = 1.0 * xScale;\n\t\tinner += this._data[ i ] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\tthis.norm += ( ( dot( x, x ) + ( ( this.intercept ) ? 1.0 : 0.0 ) ) *\n\t\tpow( xScale, 2 ) ) +\n\t\t( 2.0 * this.scale * inner );\n}\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @private\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nfunction innerProduct( x ) {\n\t/* eslint-disable no-invalid-this */\n\tvar ret = 0;\n\tvar i;\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\tret += this._data[ i ] * x[ i ];\n\t}\n\tret += ( this.intercept ) ? this._data[ i ] : 0.0;\n\tret *= this.scale;\n\treturn ret;\n}\n\n\n// MAIN //\n\n/**\n* Creates a WeightVector.\n*\n* @constructor\n* @param {PositiveInteger} dim - number of feature weights (excluding bias/intercept term)\n* @param {boolean} intercept - boolean indicating whether a bias/intercept weight should be implicitly assumed\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} second argument must be a boolean primitive\n*/\nfunction WeightVector( dim, intercept ) {\n\tvar i;\n\tif ( !(this instanceof WeightVector) ) {\n\t\treturn new WeightVector( dim, intercept );\n\t}\n\tif ( !isPositiveInteger( dim ) ) {\n\t\tthrow new TypeError( 'invalid argument. First argument `dim` must be a positive integer. Value: `' + dim + '`.' );\n\t}\n\tif ( !isBoolean( intercept ) ) {\n\t\tthrow new TypeError( 'invalid argument. Second argument `intercept` must be a boolean primitive. Value: `' + intercept + '`.' );\n\t}\n\n\tthis.scale = 1.0;\n\tthis.norm = 0.0;\n\tthis.intercept = intercept;\n\tthis.nWeights = dim + ( ( this.intercept ) ? 1 : 0 );\n\n\tthis._data = new Array( this.nWeights );\n\n\t// Initialize weights to zero:\n\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\tthis._data[ i ] = 0.0;\n\t}\n}\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @memberof WeightVector.prototype\n* @function scaleTo\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nsetReadOnly( WeightVector.prototype, 'scaleTo', scaleTo );\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @memberof WeightVector.prototype\n* @function add\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nsetReadOnly( WeightVector.prototype, 'add', add );\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @memberof WeightVector.prototype\n* @function innerProduct\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nsetReadOnly( WeightVector.prototype, 'innerProduct', innerProduct );\n\n\n// EXPORTS //\nexport default WeightVector;\n","import __var_0__ from '@stdlib/math/base/special/max';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar max = __var_0__;\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1e-7;\n\n\n// MAIN //\n\n/**\n* L2 regularization of feature weights.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} eta - current learning rate\n*/\nfunction regularize( weights, lambda, eta ) {\n\tvar scalingFactor;\n\tif ( lambda > 0.0 ) {\n\t\tscalingFactor = 1.0 - ( eta * lambda );\n\t\tweights.scaleTo( max( scalingFactor, MIN_SCALING_FACTOR ) );\n\t}\n}\n\n\n// EXPORTS //\nexport default regularize;\n","import __var_0__ from './../regularize.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar regularize = __var_0__;\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the epsilon-insensitive loss.\n*\n* ## Notes\n*\n* The penalty of the epsilon-insensitive loss is the absolute value of the dot product of the weights and `x` minus `y` whenever the absolute error exceeds epsilon, and zero otherwise.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} epsilon - insensitivity parameter\n*/\nfunction epsilonInsensitiveLoss( weights, x, y, eta, lambda, epsilon ) {\n\tvar p = weights.innerProduct( x ) - y;\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p > epsilon ) {\n\t\tweights.add( x, -eta );\n\t} else if ( p < -epsilon ) {\n\t\tweights.add( x, +eta );\n\t}\n}\n\n\n// EXPORTS //\nexport default epsilonInsensitiveLoss;\n","import __var_0__ from './../regularize.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar regularize = __var_0__;\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the squared error loss.\n*\n* ## Notes\n*\n* The squared error loss is defined as the squared difference of the observed and fitted value.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n*/\nfunction squaredErrorLoss( weights, x, y, eta, lambda ) {\n\tvar loss = y - weights.innerProduct( x );\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tweights.add( x, ( eta * loss ) );\n}\n\n\n// EXPORTS //\nexport default squaredErrorLoss;\n","import __var_0__ from './../regularize.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar regularize = __var_0__;\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the [Huber loss][1] function.\n*\n* ## Notes\n*\n* The Huber loss uses squared-error loss for observations with error smaller than epsilon in magnitude and linear loss above that in order to decrease the influence of outliers on the model fit.\n*\n* [1]: https://en.wikipedia.org/wiki/Huber_loss\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} epsilon - insensitivity parameter\n*/\nfunction huberLoss( weights, x, y, eta, lambda, epsilon ) {\n\tvar p = weights.innerProduct( x ) - y;\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p > epsilon ) {\n\t\tweights.add( x, -eta );\n\t} else if ( p < -epsilon ) {\n\t\tweights.add( x, +eta );\n\t} else {\n\t\tweights.add( x, -eta * p );\n\t}\n}\n\n\n// EXPORTS //\nexport default huberLoss;\n","\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Returns a function to retrieve the current learning rate.\n*\n* @private\n* @param {string} type - string denoting the learning rate to use. Can be `constant`, `pegasos` or `basic`.\n* @param {PositiveNumber} eta0 - constant learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @throws {Error} first argument must be `basic`, `constant` or `pegasos`\n* @returns {Function} getEta function\n*/\nfunction closure( type, eta0, lambda ) {\n\tvar iter;\n\tvar ret;\n\n\titer = 1;\n\n\tswitch ( type ) {\n\tcase 'basic':\n\t\t// Default case: 'basic'\n\t\tret = getEtaBasic;\n\t\tbreak;\n\tcase 'constant':\n\t\tret = getEtaConstant;\n\t\tbreak;\n\tcase 'pegasos':\n\t\tret = getEtaPegasos;\n\t\tbreak;\n\tdefault:\n\t\tthrow new Error( 'invalid input value. `learningRate` must be either `basic`, `constant` or `pegasos`. Value: `' + type + '`' );\n\t}\n\treturn ret;\n\n\t/**\n\t* Returns the basic learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaBasic() {\n\t\tvar eta = 1000.0 / ( iter + 1000.0 );\n\t\titer += 1;\n\t\treturn eta;\n\t}\n\n\t/**\n\t* Returns the constant learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaConstant() {\n\t\titer += 1;\n\t\treturn eta0;\n\t}\n\n\t/**\n\t* Returns the Pegasos learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaPegasos() {\n\t\tvar eta = 1.0 / ( lambda * iter );\n\t\titer += 1;\n\t\treturn eta;\n\t}\n}\n\n\n// EXPORTS //\nexport default closure;\n","import __var_0__ from '@stdlib/assert/is-nonnegative-number';\nimport __var_1__ from '@stdlib/assert/is-positive-number';\nimport __var_2__ from '@stdlib/assert/is-boolean';\nimport __var_3__ from '@stdlib/assert/is-plain-object';\nimport __var_4__ from '@stdlib/assert/is-string';\nimport __var_5__ from '@stdlib/assert/has-own-property';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar isNonNegativeNumber = __var_0__.isPrimitive;\nvar isPositiveNumber = __var_1__.isPrimitive;\nvar isBoolean = __var_2__.isPrimitive;\nvar isObject = __var_3__;\nvar isString = __var_4__.isPrimitive;\nvar hasOwnProp = __var_5__;\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.epsilon] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0] - constant learning rate\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {string} [options.learningRate] - the learning rate to use\n* @param {string} [options.loss] -  the loss function to use\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( 'invalid argument. Options must be an object. Value: `' + options + '`.' );\n\t}\n\tif ( hasOwnProp( options, 'epsilon' ) ) {\n\t\topts.epsilon = options.epsilon;\n\t\tif ( !isPositiveNumber( opts.epsilon ) ) {\n\t\t\treturn new TypeError( 'invalid option. `epsilon` option must be a positive number. Option: `' + opts.epsilon + '`.' );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'eta0' ) ) {\n\t\topts.eta0 = options.eta0;\n\t\tif ( !isPositiveNumber( opts.eta0 ) ) {\n\t\t\treturn new TypeError( 'invalid option. `eta0` option must be a positive number. Option: `' + opts.eta0 + '`.' );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( 'invalid option. `lambda` option must be a nonnegative number. Option: `' + opts.lambda + '`.' );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\topts.learningRate = options.learningRate;\n\t\tif ( !isString( opts.learningRate ) ) {\n\t\t\treturn new TypeError( 'invalid option. `learningRate` option must be a primitive string. Option: `' + opts.learningRate + '`.' );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !isString( opts.loss ) ) {\n\t\t\treturn new TypeError( 'invalid option. `loss` option must be a primitive string. Option: `' + opts.loss + '`.' );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( 'invalid option. `intercept` option must be a primitive boolean. Option: `' + opts.intercept + '`.' );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\nexport default validate;\n","import __var_0__ from '@stdlib/assert/is-array';\nimport __var_1__ from '@stdlib/utils/copy';\nimport __var_2__ from '@stdlib/utils/define-property';\nimport __var_3__ from './weight_vector.js';\nimport __var_4__ from './loss/epsilon_insensitive.js';\nimport __var_5__ from './loss/squared_error.js';\nimport __var_6__ from './loss/huber.js';\nimport __var_7__ from './eta_factory.js';\nimport __var_8__ from './defaults.json';\nimport __var_9__ from './validate.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable */ // TODO: fix me\n'use strict';\n\n// MODULES //\nvar isArray = __var_0__;\nvar copy = __var_1__;\nvar defineProperty = __var_2__;\nvar WeightVector = __var_3__;\nvar epsilonInsensitiveLoss = __var_4__;\nvar squaredErrorLoss = __var_5__;\nvar huberLoss = __var_6__;\nvar getEta = __var_7__;\nvar DEFAULTS = __var_8__;\nvar validate = __var_9__;\n\n\n// MAIN //\n\n/**\n* Online learning for regression using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* The sub-gradient of the loss function is estimated for each datum and the regression model is updated incrementally, with a decreasing learning rate and regularization of the feature weights based on L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3â€“30. doi:10.1007/s10107-010-0420-4\n*\n* @param {Object} [options] - options object\n* @param {PositiveNumber} [options.epsilon=0.1] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0=0.02] - constant learning rate\n* @param {NonNegativeNumber} [options.lambda=1e-3] - regularization parameter\n* @param {string} [options.learningRate='basic'] - string denoting the learning rate to use. Can be `constant`, `pegasos` or `basic`.\n* @param {string} [options.loss='squaredError'] - string denoting the loss function to use. Can be `leastSquares`, `epsilonInsensitive` or `huber`.\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @returns {Object} regression model\n*\n* @example\n* var onlineSGDRegression = require( '@stdlib/streams/ml/online-sgd-regression' );\n*\n* var model = onlineSGDRegression({\n*     'intercept': true\n*     'lambda': 1e-5\n* });\n*\n* // Update model as observations come in:\n* var y = 3.5;\n* var x = [ 2.3, 1.0, 5.0 ];\n* model.update( x, y );\n*\n* // Predict new observation:\n* var yHat = model.predict( x );\n*\n* // Retrieve coefficients:\n* var coefs = model.coefs;\n*/\nfunction onlineSGDRegression( options ) {\n\tvar _nFeatures;\n\tvar _lossfun;\n\tvar _weights;\n\tvar _getEta;\n\tvar model;\n\tvar opts;\n\tvar err;\n\n\topts = copy( DEFAULTS );\n\tif ( arguments.length > 0 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\t_weights = null;\n\n\t// Set loss function:\n\tswitch ( opts.loss ) {\n\tcase 'epsilonInsensitive':\n\t\t_lossfun = epsilonInsensitiveLoss;\n\tbreak;\n\tcase 'huber':\n\t\t_lossfun = huberLoss;\n\tbreak;\n\tcase 'squaredError':\n\t\t_lossfun = squaredErrorLoss;\n\tbreak;\n\tdefault:\n\t\tthrow Error( 'invalid input value. `loss` option must be either `epsilonInsensitive`, `huber` or `leastSquares`. Value: `' + opts.loss + '`' );\n\t}\n\n\t// Set learning rate:\n\t_getEta = getEta( opts.learningRate, opts.eta0, opts.lambda );\n\n\t/**\n\t* SGD regression model.\n\t*\n\t* @namespace SGDRegressionModel\n\t*/\n\tmodel = {};\n\n\t// Define coefficient getter:\n\n\t/**\n\t* Model coefficients / feature weights.\n\t*\n\t* @name coefs\n\t* @memberof SGDRegressionModel\n\t* @type {Array}\n\t*\n\t* @example\n\t* // Retrieve coefficients:\n\t* var coefs = model.coefs;\n\t*/\n\tdefineProperty( model, 'coefs', {\n\t\t'get': function getCoefs() {\n\t\t\tvar ret;\n\t\t\tvar i;\n\n\t\t\tret = new Array( _weights.nWeights );\n\t\t\tfor ( i = 0; i < ret.length; i++ ) {\n\t\t\t\tret[ i ] = _weights._data[ i ] * _weights.scale;\n\t\t\t}\n\t\t\treturn ret;\n\t\t},\n\t\t'configurable': false,\n\t\t'enumerable': false\n\t});\n\n\t/**\n\t* Update weights given new observations `y` and `x`.\n\t*\n\t* @name update\n\t* @memberof SGDRegressionModel\n\t* @type {Function}\n\t* @param {NumericArray} x - feature vector\n\t* @param {number} y - response value\n\t* @throws {TypeError} first argument must be an array\n\t* @throws {TypeError} first argument must have length equal to the number of predictors\n\t*\n\t* @example\n\t* // Update model as observations come in:\n\t* var y = 3.5;\n\t* var x = [ 2.3, 1.0, 5.0 ];\n\t* model.update( x, y );\n\t*/\n\tmodel.update = function update( x, y ) {\n\t\tvar eta;\n\n\t\tif ( !_weights ) {\n\t\t\t_weights = new WeightVector( x.length, opts.intercept );\n\t\t\t_nFeatures = opts.intercept ? _weights.nWeights - 1 : _weights.nWeights;\n\t\t}\n\n\t\tif ( !isArray( x ) || x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( 'invalid input value. First argument `x` must be an array of length ' + this.nFeatures + '. Value: `' + x + '`' );\n\t\t}\n\n\t\t// Get current learning rate...\n\t\teta = _getEta();\n\n\t\t// Update weights depending on the chosen loss function...\n\t\t_lossfun( _weights, x, y, eta, opts.lambda, opts.epsilon );\n\t}; // end METHOD _updateWithoutIntercept()\n\n\t/**\n\t* Predict response for a new observation with features `x`.\n\t*\n\t* @name predict\n\t* @memberof SGDRegressionModel\n\t* @type {Function}\n\t* @param {NumericArray} x - feature vector\n\t* @throws {TypeError} first argument must be an array\n\t* @throws {TypeError} first argument must have length equal to the number of predictors\n\t* @returns {number} response value\n\t*\n\t* @example\n\t* // Predict new observation:\n\t* var x = [ 2.3, 5.3, 8.6 ];\n\t* var yHat = model.predict( x );\n\t*/\n\tmodel.predict = function predict( x ) {\n\t\tif ( !isArray( x ) || x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( 'invalid input value. First argument `x` must be an array of length ' + this.nFeatures + '. Value: `' + x + '`' );\n\t\t}\n\t\treturn _weights.innerProduct( x );\n\t}; // end METHOD predict()\n\n\treturn model;\n}\n\n\n// EXPORTS //\nexport default onlineSGDRegression;\n","import __var_0__ from './learner.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Online learning for regression using stochastic gradient descent (SGD).\n*\n* @module @stdlib/ml/online-sgd-regression\n*\n* @example\n* var onlineSGDRegression = require( '@stdlib/ml/online-sgd-regression' );\n*\n* var model = onlineSGDRegression({\n*     'intercept': true\n*     'lambda': 1e-5\n* });\n*\n* var y = 3.5;\n* var x = [ 2.3, 1.0, 5.0 ];\n* model.update( x, y );\n*/\n\n// MODULES //\nvar onlineSGDRegression = __var_0__;\n\n\n// EXPORTS //\nexport default onlineSGDRegression;\n"]}