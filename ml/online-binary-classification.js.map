{"version":3,"sources":["@stdlib/ml/online-binary-classification/lib/regularize.js","@stdlib/ml/online-binary-classification/lib/loss/modified_huber.js","@stdlib/ml/online-binary-classification/lib/loss/squared_hinge.js","@stdlib/ml/online-binary-classification/lib/dot.js","@stdlib/ml/online-binary-classification/lib/weight_vector.js","@stdlib/ml/online-binary-classification/lib/loss/perceptron.js","@stdlib/ml/online-binary-classification/lib/loss/hinge.js","@stdlib/ml/online-binary-classification/lib/loss/log.js","@stdlib/ml/online-binary-classification/lib/eta_factory.js","@stdlib/ml/online-binary-classification/lib/validate.js","@stdlib/ml/online-binary-classification/lib/learner.js","@stdlib/ml/online-binary-classification/lib/index.js"],"names":["max","__var_0__","MIN_SCALING_FACTOR","regularize","weights","lambda","eta","scalingFactor","scaleTo","modifiedHuber","x","y","p","innerProduct","add","squaredHingeLoss","dot","i","len","length","ret","isPositiveInteger","isBoolean","__var_1__","setReadOnly","__var_2__","pow","__var_3__","__var_4__","MIN_SCALE","factor","this","scale","nWeights","_data","norm","RangeError","xScale","xscaled","inner","intercept","WeightVector","dim","TypeError","Array","prototype","perceptron","z","hingeLoss","exp","logLoss","loss","constantFunction","basic","pegasos","getEta","type","eta0","Error","isNonNegativeNumber","isPrimitive","isPositiveNumber","isObject","isString","hasOwnProp","__var_5__","validate","opts","options","epsilon","learningRate","isArray","copy","defineProperty","modifiedHuberLoss","__var_6__","__var_7__","__var_8__","__var_9__","__var_10__","__var_11__","DEFAULTS","__var_12__","__var_13__","SGDClassificationModel","_nFeatures","_lossfun","_weights","_getEta","model","err","arguments","get","configurable","enumerable","update","nFeatures","predict","wx","onlineBinaryClassification"],"mappings":"srBAsBA,IAAIA,EAAMC,EAKNC,EAAqB,KAazB,SAASC,EAAYC,EAASC,EAAQC,GACrC,IAAIC,EACCF,EAAS,IACbE,EAAgB,EAAQD,EAAMD,EAC9BD,EAAQI,QAASR,EAAKO,EAAeL,KCtBvC,IAAIC,EAAaF,EA0BjB,SAASQ,EAAeL,EAASM,EAAGC,EAAGL,EAAKD,GAC3C,IAAIO,EAAID,EAAIP,EAAQS,aAAcH,GAGlCP,EAAYC,EAASC,EAAQC,GAElB,IAANK,IACCC,GAAK,EACTR,EAAQU,IAAKJ,EAAW,EAANJ,EAAYK,GACnBC,EAAI,GACfR,EAAQU,IAAKJ,EAAGJ,GAAQK,EAAKC,EAAED,KCpClC,IAAIR,EAAaF,EAuBjB,SAASc,EAAkBX,EAASM,EAAGC,EAAGL,EAAKD,GAC9C,IAAIO,EAAID,EAAIP,EAAQS,aAAcH,GAGlCP,EAAYC,EAASC,EAAQC,GAExBM,EAAI,GAAa,IAAND,GACfP,EAAQU,IAAKJ,EAAGJ,GAAQK,EAAMC,EAAED,IChBlC,SAASK,EAAKN,EAAGC,GAChB,IAEIM,EAFAC,EAAMR,EAAES,OACRC,EAAM,EAGV,IAAMH,EAAI,EAAGA,EAAIC,EAAKD,IACrBG,GAAOV,EAAGO,GAAMN,EAAGM,GAEpB,OAAOG,ECdR,IAAIC,EAAoBpB,EACpBqB,EAAYC,EACZC,EAAcC,EACdC,EAAMC,EACNX,EAAMY,EAKNC,EAAY,MAYhB,SAASrB,EAASsB,GAEjB,IAAIb,EACJ,GAAKc,KAAKC,MAAQH,EAAY,CAE7B,IAAMZ,EAAI,EAAGA,EAAIc,KAAKE,SAAUhB,IAC/Bc,KAAKG,MAAOjB,IAAOc,KAAKC,MAEzBD,KAAKC,MAAQ,EAKd,GAFAD,KAAKI,MAAQT,EAAKI,EAAQ,KAErBA,EAAS,GAGb,MAAM,IAAIM,WAAY,sGAAwGN,EAAS,KAFvIC,KAAKC,OAASF,EAahB,SAAShB,EAAKJ,EAAG2B,GAEhB,IAAIC,EACAC,EACAtB,EAMJ,IAJAsB,EAAQ,OACQ,IAAXF,IACJA,EAAS,GAEJpB,EAAI,EAAGA,EAAIP,EAAES,OAAQF,IAC1BqB,EAAU5B,EAAGO,GAAMoB,EACnBE,GAASR,KAAKG,MAAMjB,GAAKqB,EACzBP,KAAKG,MAAOjB,GAAMc,KAAKG,MAAOjB,GAAQqB,EAAUP,KAAKC,MAGjDD,KAAKS,YACTF,EAAU,EAAMD,EAChBE,GAASR,KAAKG,MAAOjB,GAAMqB,EAC3BP,KAAKG,MAAOjB,GAAMc,KAAKG,MAAOjB,GAAQqB,EAAUP,KAAKC,OAEtDD,KAAKI,OAAYnB,EAAKN,EAAGA,IAAUqB,KAAc,UAAK,EAAM,IAC3DL,EAAKW,EAAQ,GACX,EAAMN,KAAKC,MAAQO,EAUvB,SAAS1B,EAAcH,GAEtB,IACIO,EADAG,EAAM,EAEV,IAAMH,EAAI,EAAGA,EAAIP,EAAES,OAAQF,IAC1BG,GAAOW,KAAKG,MAAOjB,GAAMP,EAAGO,GAI7B,OAFAG,GAASW,KAAc,UAAKA,KAAKG,MAAOjB,GAAM,EAC9CG,GAAOW,KAAKC,MAgBb,SAASS,EAAcC,EAAKF,GAC3B,IAAIvB,EACJ,KAAOc,gBAAgBU,GACtB,OAAO,IAAIA,EAAcC,EAAKF,GAE/B,IAAMnB,EAAmBqB,GACxB,MAAM,IAAIC,UAAW,8EAAgFD,EAAM,MAE5G,IAAMpB,EAAWkB,GAChB,MAAM,IAAIG,UAAW,sFAAwFH,EAAY,MAW1H,IARAT,KAAKC,MAAQ,EACbD,KAAKI,KAAO,EACZJ,KAAKS,UAAYA,EACjBT,KAAKE,SAAWS,GAAUX,KAAc,UAAK,EAAI,GAEjDA,KAAKG,MAAQ,IAAIU,MAAOb,KAAKE,UAGvBhB,EAAI,EAAGA,EAAIc,KAAKE,SAAUhB,IAC/Bc,KAAKG,MAAOjB,GAAM,EAYpBO,EAAaiB,EAAaI,UAAW,UAAWrC,GAUhDgB,EAAaiB,EAAaI,UAAW,MAAO/B,GAU5CU,EAAaiB,EAAaI,UAAW,eAAgBhC,GCtKrD,IAAIV,EAAaF,EAoBjB,SAAS6C,EAAY1C,EAASM,EAAGC,EAAGL,EAAKD,GACxC,IAAI0C,EAAI3C,EAAQS,aAAcH,GAG9BP,EAAYC,EAASC,EAAQC,GAExBK,EAAIoC,GAAK,GACb3C,EAAQU,IAAKJ,EAAKJ,EAAMK,GC3B1B,IAAIR,EAAaF,EAuBjB,SAAS+C,EAAW5C,EAASM,EAAGC,EAAGL,EAAKD,GACvC,IAAIO,EAAID,EAAIP,EAAQS,aAAcH,GAGlCP,EAAYC,EAASC,EAAQC,GAExBM,EAAI,GAAa,IAAND,GACfP,EAAQU,IAAKJ,EAAKJ,EAAMK,GC7B1B,IAAIsC,EAAMhD,EACNE,EAAaoB,EAuBjB,SAAS2B,EAAS9C,EAASM,EAAGC,EAAGL,EAAKD,GACrC,IAAI8C,EAAOxC,GAAM,EAAMsC,EAAKtC,EAAIP,EAAQS,aAAcH,KAGtDP,EAAYC,EAASC,EAAQC,GAE7BF,EAAQU,IAAKJ,EAAKJ,EAAM6C,GC/BzB,IAAIC,EAAmBnD,EAWvB,SAASoD,IACR,IAAIpC,EAAI,EACR,OAQA,WACC,IAAIX,EAAM,KAAWW,EAAI,KAEzB,OADAA,GAAK,EACEX,GAWT,SAASgD,EAASjD,GACjB,IAAIY,EAAI,EACR,OAQA,WACC,IAAIX,EAAM,GAAQD,EAASY,GAE3B,OADAA,GAAK,EACEX,GAiBT,SAASiD,EAAQC,EAAMC,EAAMpD,GAC5B,GAAc,UAATmD,EACJ,OAAOH,IAER,GAAc,aAATG,EACJ,OAAOJ,EAAkBK,GAE1B,GAAc,YAATD,EACJ,OAAOF,EAASjD,GAEjB,MAAM,IAAIqD,MAAO,mFAAqFF,EAAO,mHCtE1GG,EAAsB1D,EAAU2D,YAChCC,EAAmBtC,EAAUqC,YAC7BtC,EAAYG,EAAUmC,YACtBE,GAAWnC,EACXoC,GAAWnC,EAAUgC,YACrBI,GAAaC,EA2BjB,SAASC,GAAUC,EAAMC,GACxB,OAAMN,GAAUM,GAGXJ,GAAYI,EAAS,aACzBD,EAAKE,QAAUD,EAAQC,SACjBR,EAAkBM,EAAKE,UACrB,IAAI1B,UAAW,wEAA0EwB,EAAKE,QAAU,MAG5GL,GAAYI,EAAS,UACzBD,EAAKV,KAAOW,EAAQX,MACdI,EAAkBM,EAAKV,OACrB,IAAId,UAAW,qEAAuEwB,EAAKV,KAAO,MAGtGO,GAAYI,EAAS,YACzBD,EAAK9D,OAAS+D,EAAQ/D,QAChBsD,EAAqBQ,EAAK9D,SACxB,IAAIsC,UAAW,0EAA4EwB,EAAK9D,OAAS,MAG7G2D,GAAYI,EAAS,kBACzBD,EAAKG,aAAeF,EAAQE,cACtBP,GAAUI,EAAKG,eACb,IAAI3B,UAAW,8EAAgFwB,EAAKG,aAAe,MAGvHN,GAAYI,EAAS,UACzBD,EAAKhB,KAAOiB,EAAQjB,MACdY,GAAUI,EAAKhB,OACb,IAAIR,UAAW,sEAAwEwB,EAAKhB,KAAO,MAGvGa,GAAYI,EAAS,eACzBD,EAAK3B,UAAY4B,EAAQ5B,WACnBlB,EAAW6C,EAAK3B,YACd,IAAIG,UAAW,4EAA8EwB,EAAK3B,UAAY,MAGhH,KAtCC,IAAIG,UAAW,wDAA0DyB,EAAU,MCzB5F,IAAIL,GAAW9D,EAAU2D,YACrBW,GAAUhD,EACViD,GAAO/C,EACPgD,GAAiB9C,EACjBsB,GAAMrB,EACN8C,GAAoBT,EACpBlD,GAAmB4D,EACnBlC,GAAemC,EACf9B,GAAa+B,EACb7B,GAAY8B,EACZ5B,GAAU6B,EACVxB,GAASyB,EACTC,GAAWC,EACXhB,GAAWiB,GA6Cf,SAASC,GAAwBhB,GAChC,IAAIiB,EACAC,EACAC,EACAC,EACAC,EACAtB,EACAuB,EAIJ,GADAvB,EAAOK,GAAMS,IACRU,UAAUxE,OAAS,IACvBuE,EAAMxB,GAAUC,EAAMC,IAErB,MAAMsB,EAQR,OALAH,EAAW,KAKFpB,EAAKhB,MACd,IAAK,MACJmC,EAAWpC,GACZ,MACA,IAAK,QACJoC,EAAWtC,GACZ,MACA,IAAK,gBACJsC,EAAWZ,GACZ,MACA,IAAK,aACJY,EAAWxC,GACZ,MACA,IAAK,eACJwC,EAAWvE,GACZ,MACA,QACC,MAAM2C,MAAO,8HAAgIS,EAAKhB,KAAO,KAmH1J,OA/GAqC,EAAUjC,GAAQY,EAAKG,aAAcH,EAAKV,KAAMU,EAAK9D,QAsBrDoE,GAfAgB,EAAQ,GAee,QAAS,CAC/BG,IAAO,WACN,IAAIxE,EACAH,EAGJ,IADAG,EAAM,IAAIwB,MAAO2C,EAAStD,UACpBhB,EAAI,EAAGA,EAAIG,EAAID,OAAQF,IAC5BG,EAAKH,GAAMsE,EAASrD,MAAOjB,GAAMsE,EAASvD,MAE3C,OAAOZ,GAERyE,cAAgB,EAChBC,YAAc,IAkBfL,EAAMM,OAAS,SAAiBrF,EAAGC,GAClC,IAAIL,EAOJ,GALMiF,IACLA,EAAW,IAAI9C,GAAc/B,EAAES,OAAQgD,EAAK3B,WAC5C6C,EAAalB,EAAK3B,UAAY+C,EAAStD,SAAW,EAAIsD,EAAStD,WAG1DsC,GAAS7D,IAAOA,EAAES,SAAWkE,EAClC,MAAM,IAAI1C,UAAW,sEAAwEZ,KAAKiE,UAAY,aAAetF,EAAI,KAElI,IAAY,IAAPC,GAAkB,IAANA,EAChB,MAAM,IAAIgC,UAAW,sEAAwEhC,EAAI,KAIlGL,EAAMkF,IAGNF,EAAUC,EAAU7E,EAAGC,EAAGL,EAAK6D,EAAK9D,SAkBrCoF,EAAMQ,QAAU,SAAkBvF,EAAG8C,GACpC,IAAI0C,EACJ,IAAM3B,GAAS7D,IAAOA,EAAES,SAAWkE,EAClC,MAAM,IAAI1C,UAAW,sEAAwEZ,KAAKiE,UAAY,aAAetF,EAAI,KAElI,GAAKiF,UAAUxE,OAAS,KACjB4C,GAAUP,IAAqB,SAATA,GAA4B,gBAATA,GAC9C,MAAM,IAAIb,UAAW,wFAA0FjC,EAAI,KAIrH,GADAwF,EAAKX,EAAS1E,aAAcH,GACd,gBAAT8C,EACJ,OAAO0C,EAGR,GAAKZ,IAAapC,IAAWoC,IAAaZ,GACzC,MAAM,IAAIhB,MAAO,+FAAiGF,EAAO,KAG1H,OAAO,GAAQ,EAAMP,IAAMiD,KAGrBT,EC5ML,IAACU,GAA6BlG","sourcesContent":["import __var_0__ from '@stdlib/math/base/special/max';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar max = __var_0__;\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1e-7;\n\n\n// MAIN //\n\n/**\n* L2 regularization of feature weights.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} eta - current learning rate\n*/\nfunction regularize( weights, lambda, eta ) {\n\tvar scalingFactor;\n\tif ( lambda > 0.0 ) {\n\t\tscalingFactor = 1.0 - ( eta * lambda );\n\t\tweights.scaleTo( max( scalingFactor, MIN_SCALING_FACTOR ) );\n\t}\n}\n\n\n// EXPORTS //\nexport default regularize;\n","import __var_0__ from './../regularize.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar regularize = __var_0__;\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the modified Huber loss.\n*\n* ## Notes\n*\n* The modified Huber loss is defined as\n*\n* ```tex\n* \\begin{cases}\n*     \\max(0,1-y\\,f(x))^{2} & {\\textrm{for}}\\,\\,y\\,f(x)\\geq -1,\\\\\n*     -4y\\,f(x) & {\\textrm{otherwise.}}\n* \\end{cases}\n* ```\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n*/\nfunction modifiedHuber( weights, x, y, eta, lambda ) {\n\tvar p = y * weights.innerProduct( x );\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( y !== 0.0 ) {\n\t\tif ( p < -1.0 ) {\n\t\t\tweights.add( x, ( eta * 4.0 * y ) );\n\t\t} else if ( p < 1.0 ) {\n\t\t\tweights.add( x, eta * ( y - (p*y) ) );\n\t\t}\n\t}\n}\n\n\n// EXPORTS //\nexport default modifiedHuber;\n","import __var_0__ from './../regularize.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar regularize = __var_0__;\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the squared hinge loss.\n*\n* ## Notes\n*\n* The squared hinge loss is defined as\n*\n* ```tex\n* \\max\\{ 0, 1 - y w^\\intercal x \\}^2\n* ```\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n*/\nfunction squaredHingeLoss( weights, x, y, eta, lambda ) {\n\tvar p = y * weights.innerProduct( x );\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p < 1.0 && y !== 0.0 ) {\n\t\tweights.add( x, eta * ( y - ( p*y ) ) );\n\t}\n}\n\n\n// EXPORTS //\nexport default squaredHingeLoss;\n","\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Calculates the dot product of two vectors.\n*\n* @private\n* @param {NumericArray} x - first vector\n* @param {NumericArray} y - second vector\n* @returns {number} dot product\n*\n* @example\n* var x = [ 1.0, 2.0, 3.0 ];\n* var y = [ 1.0, 2.0, 2.0 ];\n*\n* var ret = dot( x, y );\n* // returns 11.0\n*/\nfunction dot( x, y ) {\n\tvar len = x.length;\n\tvar ret = 0;\n\tvar i;\n\n\tfor ( i = 0; i < len; i++ ) {\n\t\tret += x[ i ] * y[ i ];\n\t}\n\treturn ret;\n}\n\n\n// EXPORTS //\nexport default dot;\n","import __var_0__ from '@stdlib/assert/is-positive-integer';\nimport __var_1__ from '@stdlib/assert/is-boolean';\nimport __var_2__ from '@stdlib/utils/define-nonenumerable-read-only-property';\nimport __var_3__ from '@stdlib/math/base/special/pow';\nimport __var_4__ from './dot.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/*\n* The weight vector implementation was inspired by the [sofia-ml]{@link https://code.google.com/archive/p/sofia-ml/} library.\n*/\n\n// MODULES //\nvar isPositiveInteger = __var_0__;\nvar isBoolean = __var_1__;\nvar setReadOnly = __var_2__;\nvar pow = __var_3__;\nvar dot = __var_4__;\n\n\n// VARIABLES //\n\nvar MIN_SCALE = 1.0e-11;\n\n\n// FUNCTIONS //\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @private\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nfunction scaleTo( factor ) {\n\t/* eslint-disable no-invalid-this */\n\tvar i;\n\tif ( this.scale < MIN_SCALE ) {\n\t\t// Scale vector to one:\n\t\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\t\tthis._data[ i ] *= this.scale;\n\t\t}\n\t\tthis.scale = 1.0;\n\t}\n\n\tthis.norm *= pow( factor, 2 );\n\n\tif ( factor > 0.0 ) {\n\t\tthis.scale *= factor;\n\t} else {\n\t\tthrow new RangeError( 'Scaling weight vector by nonpositive value, likely due to too large value of eta * lambda. Value: `' + factor + '`' );\n\t}\n}\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @private\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nfunction add( x, xScale ) {\n\t/* eslint-disable no-invalid-this */\n\tvar xscaled;\n\tvar inner;\n\tvar i;\n\n\tinner = 0.0;\n\tif ( xScale === void 0 ) {\n\t\txScale = 1.0;\n\t}\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\txscaled = x[ i ] * xScale;\n\t\tinner += this._data[i] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this.intercept ) {\n\t\txscaled = 1.0 * xScale;\n\t\tinner += this._data[ i ] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\tthis.norm += ( ( dot( x, x ) + ( ( this.intercept ) ? 1.0 : 0.0 ) ) *\n\t\tpow( xScale, 2 ) ) +\n\t\t( 2.0 * this.scale * inner );\n}\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @private\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nfunction innerProduct( x ) {\n\t/* eslint-disable no-invalid-this */\n\tvar ret = 0;\n\tvar i;\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\tret += this._data[ i ] * x[ i ];\n\t}\n\tret += ( this.intercept ) ? this._data[ i ] : 0.0;\n\tret *= this.scale;\n\treturn ret;\n}\n\n\n// MAIN //\n\n/**\n* Creates a WeightVector.\n*\n* @constructor\n* @param {PositiveInteger} dim - number of feature weights (excluding bias/intercept term)\n* @param {boolean} intercept - boolean indicating whether a bias/intercept weight should be implicitly assumed\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} second argument must be a boolean primitive\n*/\nfunction WeightVector( dim, intercept ) {\n\tvar i;\n\tif ( !(this instanceof WeightVector) ) {\n\t\treturn new WeightVector( dim, intercept );\n\t}\n\tif ( !isPositiveInteger( dim ) ) {\n\t\tthrow new TypeError( 'invalid argument. First argument `dim` must be a positive integer. Value: `' + dim + '`.' );\n\t}\n\tif ( !isBoolean( intercept ) ) {\n\t\tthrow new TypeError( 'invalid argument. Second argument `intercept` must be a boolean primitive. Value: `' + intercept + '`.' );\n\t}\n\n\tthis.scale = 1.0;\n\tthis.norm = 0.0;\n\tthis.intercept = intercept;\n\tthis.nWeights = dim + ( ( this.intercept ) ? 1 : 0 );\n\n\tthis._data = new Array( this.nWeights );\n\n\t// Initialize weights to zero:\n\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\tthis._data[ i ] = 0.0;\n\t}\n}\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @memberof WeightVector.prototype\n* @function scaleTo\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nsetReadOnly( WeightVector.prototype, 'scaleTo', scaleTo );\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @memberof WeightVector.prototype\n* @function add\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nsetReadOnly( WeightVector.prototype, 'add', add );\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @memberof WeightVector.prototype\n* @function innerProduct\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nsetReadOnly( WeightVector.prototype, 'innerProduct', innerProduct );\n\n\n// EXPORTS //\nexport default WeightVector;\n","import __var_0__ from './../regularize.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar regularize = __var_0__;\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the perceptron loss.\n*\n* ## Notes\n*\n* -   The perceptron loss is equal to the hinge loss without a margin\n* -   The perceptron loss will not update model parameters when the response is correctly classified\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n*/\nfunction perceptron( weights, x, y, eta, lambda ) {\n\tvar z = weights.innerProduct( x );\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( y * z <= 0 ) {\n\t\tweights.add( x, ( eta * y ) );\n\t}\n}\n\n\n// EXPORTS //\nexport default perceptron;\n","import __var_0__ from './../regularize.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar regularize = __var_0__;\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the hinge loss.\n*\n* ## Notes\n*\n* The hinge loss is defined as\n*\n* ```tex\n* \\max\\{ 0, 1 - y w^\\intercal x \\}\n* ```\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n*/\nfunction hingeLoss( weights, x, y, eta, lambda ) {\n\tvar p = y * weights.innerProduct( x );\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p < 1.0 && y !== 0.0 ) {\n\t\tweights.add( x, ( eta * y ) );\n\t}\n}\n\n\n// EXPORTS //\nexport default hingeLoss;\n","import __var_0__ from '@stdlib/math/base/special/exp';\nimport __var_1__ from './../regularize.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar exp = __var_0__;\nvar regularize = __var_1__;\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the log loss.\n*\n* ## Notes\n*\n* The log loss is defined as\n*\n* ```tex\n* \\ln( 1 + \\exp( -y w^\\intercal x ) )\n* ```\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n*/\nfunction logLoss( weights, x, y, eta, lambda ) {\n\tvar loss = y / ( 1.0 + exp( y * weights.innerProduct( x ) ) );\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tweights.add( x, ( eta * loss ) );\n}\n\n\n// EXPORTS //\nexport default logLoss;\n","import __var_0__ from '@stdlib/utils/constant-function';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar constantFunction = __var_0__;\n\n\n// FUNCTIONS //\n\n/**\n* Returns a function to retrieve a learning rate.\n*\n* @private\n* @returns {Function} function to retrieve learning rate\n*/\nfunction basic() {\n\tvar i = 1;\n\treturn getEta;\n\n\t/**\n\t* Returns a learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEta() {\n\t\tvar eta = 1000.0 / ( i + 1000.0 );\n\t\ti += 1;\n\t\treturn eta;\n\t}\n}\n\n/**\n* Returns a function to retrieve a learning rate.\n*\n* @private\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @returns {Function} function to retrieve learning rate\n*/\nfunction pegasos( lambda ) {\n\tvar i = 1;\n\treturn getEta;\n\n\t/**\n\t* Returns a learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEta() {\n\t\tvar eta = 1.0 / ( lambda * i );\n\t\ti += 1;\n\t\treturn eta;\n\t}\n}\n\n\n// MAIN //\n\n/**\n* Returns a function to retrieve the current learning rate.\n*\n* @private\n* @param {string} type - string denoting the learning rate to use\n* @param {PositiveNumber} eta0 - constant learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @throws {Error} first argument must be a recognized learning rate\n* @returns {Function} function to retrieve current learning rate\n*/\nfunction getEta( type, eta0, lambda ) {\n\tif ( type === 'basic' ) {\n\t\treturn basic();\n\t}\n\tif ( type === 'constant' ) {\n\t\treturn constantFunction( eta0 );\n\t}\n\tif ( type === 'pegasos' ) {\n\t\treturn pegasos( lambda );\n\t}\n\tthrow new Error( 'invalid input value. First argument must be a recognized learning rate. Value: `' + type + '`.' );\n}\n\n\n// EXPORTS //\nexport default getEta;\n","import __var_0__ from '@stdlib/assert/is-nonnegative-number';\nimport __var_1__ from '@stdlib/assert/is-positive-number';\nimport __var_2__ from '@stdlib/assert/is-boolean';\nimport __var_3__ from '@stdlib/assert/is-plain-object';\nimport __var_4__ from '@stdlib/assert/is-string';\nimport __var_5__ from '@stdlib/assert/has-own-property';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\nvar isNonNegativeNumber = __var_0__.isPrimitive;\nvar isPositiveNumber = __var_1__.isPrimitive;\nvar isBoolean = __var_2__.isPrimitive;\nvar isObject = __var_3__;\nvar isString = __var_4__.isPrimitive;\nvar hasOwnProp = __var_5__;\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.epsilon] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0] - constant learning rate\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {string} [options.learningRate] - the learning rate to use\n* @param {string} [options.loss] -  the loss function to use\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( 'invalid argument. Options must be an object. Value: `' + options + '`.' );\n\t}\n\tif ( hasOwnProp( options, 'epsilon' ) ) {\n\t\topts.epsilon = options.epsilon;\n\t\tif ( !isPositiveNumber( opts.epsilon ) ) {\n\t\t\treturn new TypeError( 'invalid option. `epsilon` option must be a positive number. Option: `' + opts.epsilon + '`.' );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'eta0' ) ) {\n\t\topts.eta0 = options.eta0;\n\t\tif ( !isPositiveNumber( opts.eta0 ) ) {\n\t\t\treturn new TypeError( 'invalid option. `eta0` option must be a positive number. Option: `' + opts.eta0 + '`.' );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( 'invalid option. `lambda` option must be a nonnegative number. Option: `' + opts.lambda + '`.' );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\topts.learningRate = options.learningRate;\n\t\tif ( !isString( opts.learningRate ) ) {\n\t\t\treturn new TypeError( 'invalid option. `learningRate` option must be a primitive string. Option: `' + opts.learningRate + '`.' );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !isString( opts.loss ) ) {\n\t\t\treturn new TypeError( 'invalid option. `loss` option must be a primitive string. Option: `' + opts.loss + '`.' );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( 'invalid option. `intercept` option must be a primitive boolean. Option: `' + opts.intercept + '`.' );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\nexport default validate;\n","import __var_0__ from '@stdlib/assert/is-string';\nimport __var_1__ from '@stdlib/assert/is-array';\nimport __var_2__ from '@stdlib/utils/copy';\nimport __var_3__ from '@stdlib/utils/define-property';\nimport __var_4__ from '@stdlib/math/base/special/exp';\nimport __var_5__ from './loss/modified_huber.js';\nimport __var_6__ from './loss/squared_hinge.js';\nimport __var_7__ from './weight_vector.js';\nimport __var_8__ from './loss/perceptron.js';\nimport __var_9__ from './loss/hinge.js';\nimport __var_10__ from './loss/log.js';\nimport __var_11__ from './eta_factory.js';\nimport __var_12__ from './defaults.json';\nimport __var_13__ from './validate.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable */ // TODO: fix me\n'use strict';\n\n// MODULES //\nvar isString = __var_0__.isPrimitive;\nvar isArray = __var_1__;\nvar copy = __var_2__;\nvar defineProperty = __var_3__;\nvar exp = __var_4__;\nvar modifiedHuberLoss = __var_5__;\nvar squaredHingeLoss = __var_6__;\nvar WeightVector = __var_7__;\nvar perceptron = __var_8__;\nvar hingeLoss = __var_9__;\nvar logLoss = __var_10__;\nvar getEta = __var_11__;\nvar DEFAULTS = __var_12__;\nvar validate = __var_13__;\n\n\n// MAIN //\n\n/**\n* Online learning for classification using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* The sub-gradient of the loss function is estimated for each datum and the classification model is updated incrementally, with a decreasing learning rate and regularization of the feature weights based on L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3â€“30. doi:10.1007/s10107-010-0420-4\n*\n* @param {Object} [options] - options object\n* @param {PositiveNumber} [options.epsilon=0.1] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0=0.02] - constant learning rate\n* @param {PositiveNumber} [options.lambda=1e-3] - regularization parameter\n* @param {string} [options.learningRate='basic'] - string denoting the learning rate to use. Can be `constant`, `pegasos` or `basic`.\n* @param {string} [options.loss='log'] - string denoting the loss function to use. Can be `leastSquares`, `epsilonInsensitive` or `huber`.\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} must provide valid options\n* @returns {Object} classification model\n*\n* @example\n* var SGDClassificationModel = require( '@stdlib/streams/ml/online-sgd-classification' );\n*\n* var model = SGDClassificationModel({\n*    'intercept': true\n*    'lambda': 1e-5\n* });\n*\n* // Update model as observations come in:\n* var y = 3.5;\n* var x = [ 2.3, 1.0, 5.0 ];\n* model.update( x, y );\n*\n* // Predict new observation:\n* var yHat = model.predict( x );\n*\n* // Retrieve coefficients:\n* var coefs = model.coefs;\n*/\nfunction SGDClassificationModel( options ) {\n\tvar _nFeatures;\n\tvar _lossfun;\n\tvar _weights;\n\tvar _getEta;\n\tvar model;\n\tvar opts;\n\tvar err;\n\tvar _it;\n\n\topts = copy( DEFAULTS );\n\tif ( arguments.length > 0 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\t_weights = null;\n\t// Initialize counter:\n\t_it = 1;\n\n\t// Set loss function depending on chosen model:\n\tswitch ( opts.loss ) {\n\tcase 'log':\n\t\t_lossfun = logLoss;\n\tbreak;\n\tcase 'hinge':\n\t\t_lossfun = hingeLoss;\n\tbreak;\n\tcase 'modifiedHuber':\n\t\t_lossfun = modifiedHuberLoss;\n\tbreak;\n\tcase 'perceptron':\n\t\t_lossfun = perceptron;\n\tbreak;\n\tcase 'squaredHinge':\n\t\t_lossfun = squaredHingeLoss;\n\tbreak;\n\tdefault:\n\t\tthrow Error( 'invalid input value. `loss` option must be either `hinge`, `log`, `modifiedHuber`, `perceptron` or `squaredHinge`. Value: `' + opts.loss + '`' );\n\t}\n\n\t// Set learning rate:\n\t_getEta = getEta( opts.learningRate, opts.eta0, opts.lambda );\n\n\t/**\n\t* SGD classification model.\n\t*\n\t* @namespace SGDClassificationModel\n\t*/\n\tmodel = {};\n\n\t// Define coefficient getter:\n\n\t/**\n\t*  Model coefficients / feature weights.\n\t*\n\t* @name coefs\n\t* @memberof SGDClassificationModel\n\t* @type {Array}\n\t*\n\t* @example\n\t* // Retrieve coefficients:\n\t* var coefs = model.coefs;\n\t*/\n\tdefineProperty( model, 'coefs', {\n\t\t'get': function getCoefs() {\n\t\t\tvar ret;\n\t\t\tvar i;\n\n\t\t\tret = new Array( _weights.nWeights );\n\t\t\tfor ( i = 0; i < ret.length; i++ ) {\n\t\t\t\tret[ i ] = _weights._data[ i ] * _weights.scale;\n\t\t\t}\n\t\t\treturn ret;\n\t\t},\n\t\t'configurable': false,\n\t\t'enumerable': false\n\t});\n\n\t/**\n\t* Update weights given new observations `y` and `x`.\n\t*\n\t* @name update\n\t* @memberof SGDClassificationModel\n\t* @type {Function}\n\t* @param {NumericArray} x - feature vector\n\t* @param {number} y - response value\n\t*\n\t* @example\n\t* // Update model as observations come in:\n\t* var y = 1;\n\t* var x = [ 2.3, 1.0, 5.0 ];\n\t* model.update( x, y );\n\t*/\n\tmodel.update = function update( x, y ) {\n\t\tvar eta;\n\n\t\tif ( !_weights ) {\n\t\t\t_weights = new WeightVector( x.length, opts.intercept );\n\t\t\t_nFeatures = opts.intercept ? _weights.nWeights - 1 : _weights.nWeights;\n\t\t}\n\n\t\tif ( !isArray( x ) || x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( 'invalid input value. First argument `x` must be an array of length ' + this.nFeatures + '. Value: `' + x + '`' );\n\t\t}\n\t\tif ( y !== -1 && y !== 1 ) {\n\t\t\tthrow new TypeError( 'invalid input value. Second argument `y` must be +1 or -1. Value: `' + y + '`' );\n\t\t}\n\n\t\t// Get current learning rate...\n\t\teta = _getEta();\n\n\t\t// Update weights depending on the chosen loss function...\n\t\t_lossfun( _weights, x, y, eta, opts.lambda );\n\t}; // end METHOD _updateWithoutIntercept()\n\n\t/**\n\t* Predict response for a new observation with features `x`.\n\t*\n\t* @name predict\n\t* @memberof SGDClassificationModel\n\t* @type {Function}\n\t* @param {NumericArray} x - feature vector\n\t* @param {string} [type=\"link\"] - `probability` or `link`\n\t* @returns {number} response value\n\t*\n\t* @example\n\t* // Predict new observation:\n\t* var x = [ 2.3, 5.3, 8.6 ];\n\t* var yHat = model.predict( x );\n\t*/\n\tmodel.predict = function predict( x, type ) {\n\t\tvar wx;\n\t\tif ( !isArray( x ) || x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( 'invalid input value. First argument `x` must be an array of length ' + this.nFeatures + '. Value: `' + x + '`' );\n\t\t}\n\t\tif ( arguments.length > 1 ) {\n\t\t\tif ( !isString( type ) || ( type !== 'link' && type !== 'probability' ) ) {\n\t\t\t\tthrow new TypeError( 'invalid input value. Second argument `type` must be `probability` or `link`. Value: `' + x + '`' );\n\t\t\t}\n\t\t}\n\t\twx = _weights.innerProduct( x );\n\t\tif ( type !== 'probability' ) {\n\t\t\treturn wx;\n\t\t}\n\t\t// Case: type === 'probability'\n\t\tif ( _lossfun !== logLoss && _lossfun !== modifiedHuberLoss ) {\n\t\t\tthrow new Error( 'probability predictions are only supported when `loss` is `log` or `modifiedHuber`. Value: `' + type + '`' );\n\t\t}\n\n\t\treturn 1.0 / ( 1.0 + exp( -wx ) );\n\t}; // end METHOD predict()\n\n\treturn model;\n}\n\n\n// EXPORTS //\nexport default SGDClassificationModel;\n","import __var_0__ from './learner.js';\n/**\n\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Online learning for binary classification.\n*\n* @module @stdlib/ml/online-binary-classification\n*\n* @example\n* var onlineBinaryClassification = require( '@stdlib/ml/online-binary-classification' );\n*\n* var model = onlineBinaryClassification({\n*     'intercept': true\n*     'lambda': 1e-5\n* });\n*\n* var y = 1;\n* var x = [ 2.3, 1.0, 5.0 ];\n* model.update( x, y );\n*\n* var y = -1;\n* var x = [ 1.3, 0.2, -2.0 ];\n* model.update( x, y );\n*/\n\n// MODULES //\nvar onlineBinaryClassification = __var_0__;\n\n\n// EXPORTS //\nexport default onlineBinaryClassification;\n"]}